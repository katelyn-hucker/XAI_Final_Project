{"cells": [{"cell_type": "markdown", "metadata": {"id": "kdCIsd9IkTMi"}, "source": ["# Explainable AI Final Project CAM Implmentation and CAM Occluded Cone Analysis\n", "### Katie Hucker (kh509)\n", "\n", "In this notebook we have the implmentation code to run CAM interpretations for Nuscenes traffic cone images. The CAM interpretations are ran on a finetuned YOLOv8 model that was trained to detect the traffic cones within the dataset.\n", "\n", "The first section defines the code: functions, process, the HOW we get the CAM interpretations.\n", "\n", "Then we compare 2 visibility level traffic cones: occluded (1-40% visible) and visible (80-100%). This is important to my Capstone groups analysis of how YOLO performs for very occluded traffic cones.  "]}, {"cell_type": "markdown", "metadata": {"id": "9sDxTmg-PxA_"}, "source": ["[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1s8mPbGM1qWdGn4XyP372Eam2KCHGJ2t1?usp=sharing)"]}, {"cell_type": "markdown", "metadata": {"id": "sp9Muq6t3-AK"}, "source": ["# Section 1: The Code -- How can we create the CAM analysis?\n", "\n", "This section breaks down the functions created and used, as well, descriptions and implmentation how-to comments."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install ultralytics\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import cv2\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import torch\n", "from ultralytics import YOLO"]}, {"cell_type": "markdown", "metadata": {"id": "bUB52MurfLUy"}, "source": ["## Set-Up Code"]}, {"cell_type": "markdown", "metadata": {"id": "oO-9BjAchXy2"}, "source": ["This function prepares an image for model input by converting it to a PyTorch tensor, handling both numpy arrays and tensors. It transposes the color channel order from HWC (height, width, channels) to CHW, normalizes pixel values to [0, 1], and adds a batch dimension."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def preprocess_image(input_img):\n", "\n", "\n", "    if len(input_img.shape) == 3 and input_img.shape[2] == 3:  # HWC format\n", "        input_tensor = torch.from_numpy(input_img.transpose(2, 0, 1)).unsqueeze(0).float()\n", "    else:\n", "        input_tensor = torch.from_numpy(input_img).unsqueeze(0).float()\n", "\n", "    # Normalize to [0, 1]\n", "    if input_tensor.max() > 1:\n", "        input_tensor = input_tensor / 255.0\n", "\n", "    return input_tensor"]}, {"cell_type": "markdown", "metadata": {"id": "ke0wEvQehcXz"}, "source": ["This function loads and runs a YOLO model on a given image. It reads the image using OpenCV, resizes it to the YOLO-compatible size (640x640), converts it to RGB format, and performs inference using the YOLO model.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_model(model_path, img_path):\n", "\n", "    img = cv2.imread(img_path)\n", "    img = cv2.resize(img, (640, 640))\n", "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "\n", "    model = YOLO(model_path)\n", "    # Run inference\n", "\n", "    with torch.no_grad():\n", "        results = model(rgb_img)\n", "\n", "    return model, results"]}, {"cell_type": "markdown", "metadata": {"id": "idNty09lfNLy"}, "source": ["## CAM Code"]}, {"cell_type": "markdown", "metadata": {"id": "H7E4soJNjhh4"}, "source": ["This function extracts intermediate feature maps from a specified layer in the YOLO model using a forward hook. It preprocesses the input image, sends it through the model, and captures the output of the given layer."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def extract_feature_maps(model, layer, input_img):\n", "\n", "  #followed process described by Zhou 2015 CAM Paper cited below\n", "  #used lots of other resources all cited below\n", "\n", "    # Preprocess image\n", "    input_tensor = preprocess_image(input_img)\n", "\n", "    # Move to device\n", "    device = next(model.model.parameters()).device\n", "    input_tensor = input_tensor.to(device)\n", "\n", "    # Forward hook\n", "    feature_maps = []\n", "\n", "    def hook_fn(module, input, output):\n", "        feature_maps.append(output.detach().cpu())\n", "\n", "    handle = layer.register_forward_hook(hook_fn)\n", "\n", "    # Run model\n", "    with torch.no_grad():\n", "        model(input_tensor)\n", "\n", "    # Remove hook\n", "    handle.remove()\n", "\n", "    # Return feature maps if any were captured\n", "    if feature_maps:\n", "        return feature_maps[0]\n", "\n", "    return None"]}, {"cell_type": "markdown", "metadata": {"id": "1XimdifjjQkA"}, "source": ["This function creates a visual activation map from a tensor of feature maps\u2014typically extracted from an intermediate layer of a neural network like YOLO. It sums the absolute values across the channel dimension to capture general feature intensity, resizes the result to the original image\u2019s shape, and normalizes it to a [0, 1] range. This map highlights areas of the image that strongly activate the network and is useful in CAM-style visualizations to understand model focus areas."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def generate_activation_map(feature_maps, input_img_shape):\n", "\n", "  #followed process described by Zhou 2015 CAM Paper cited below\n", "  #used lots of other resources all cited below\n", "\n", "    # check dimensions\n", "    if feature_maps.dim() == 4:  # BCHW\n", "        feature_map = feature_maps[0]  # CHW\n", "    else:\n", "        feature_map = feature_maps\n", "\n", "    # Generate activation map by summing across channels\n", "    activation_map = torch.sum(torch.abs(feature_map), dim=0).numpy()\n", "\n", "    # Resize to input image size\n", "    h, w = input_img_shape[:2]\n", "    activation_map = cv2.resize(activation_map, (w, h))\n", "\n", "    # Normalize\n", "    activation_map = activation_map - np.min(activation_map)\n", "    activation_map = activation_map / (np.max(activation_map) + 1e-7)\n", "\n", "    return activation_map"]}, {"cell_type": "markdown", "metadata": {"id": "ss-g6G30jAYd"}, "source": ["This function calls the functions above and returns the activation map given the found features in each layer."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_cam(model, layer, input_img):\n", "    feature_maps = extract_feature_maps(model, layer, input_img)\n", "\n", "\n", "    print(f\"Feature maps shape: {feature_maps.shape}\")\n", "\n", "    # Generate activation map\n", "    activation_map = generate_activation_map(feature_maps, input_img.shape)\n", "    print(f\"Created activation map with shape {activation_map.shape}\")\n", "\n", "    return activation_map\n"]}, {"cell_type": "markdown", "metadata": {"id": "ypY1Go2SdIf7"}, "source": ["## Plotting Code"]}, {"cell_type": "markdown", "metadata": {"id": "fAyGCFsihgj3"}, "source": ["This function visualizes a Class Activation Map (CAM) by applying a heatmap over the input image to highlight the most activated regions. It normalizes the CAM mask, applies a colormap, and blends it with the original image."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def visualize_cam(img, mask, use_rgb=False, colormap=cv2.COLORMAP_JET, image_weight=0.5):\n", "\n", "    if mask.ndim > 2: #Claude Sonnet 3.7 was used to generate this if statement 4/15/2025\n", "        mask = mask[0]\n", "\n", "    # [0, 1] range\n", "    mask = mask - mask.min()\n", "    mask = mask / (mask.max() + 1e-7)\n", "\n", "    # Convert to uint8 needed for colormap\n", "    mask_uint8 = np.uint8(255 * mask)\n", "\n", "    heatmap = cv2.applyColorMap(mask_uint8, colormap) #https://docs.opencv.org/4.x/d3/d50/group__imgproc__colormap.html\n", "    if use_rgb:\n", "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n", "    heatmap = np.float32(heatmap) / 255\n", "\n", "    #keep in [0, 1] range\n", "    if np.max(img) > 1:\n", "        img = img / 255.0\n", "\n", "    # Apply the heatmap above the original image\n", "    cam = (1 - image_weight) * heatmap + image_weight * img #Claude 3.7 was used to generate this line of code on 4/15/2025\n", "    cam = cam / np.max(cam)\n", "    return np.uint8(255 * cam)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_grid(original_img, detection_img, cam_results, output_dir, filename=\"comparison.jpg\"):\n", "    \"\"\"\n", "    Make a grid of all the image outputs\n", "    \"\"\"\n", "    rows = 2\n", "    cols = 3\n", "\n", "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n", "    axes = axes.flatten()\n", "\n", "    axes[0].imshow(original_img)\n", "    axes[0].set_title(\"Original Image\")\n", "    axes[0].axis('off')\n", "\n", "\n", "    axes[1].imshow(detection_img)\n", "    axes[1].set_title(\"YOLO Detections\")\n", "    axes[1].axis('off')\n", "\n", "    for i, result in enumerate(cam_results[:4]):   # Claude 3.7 was used to generate this for loop on 4/14\n", "        axes[i+2].imshow(result['image'])\n", "        axes[i+2].set_title(f\"Layer {result['layer_idx']}: {result['layer_name']}\")\n", "        axes[i+2].axis('off')\n", "\n", "    for i in range(len(cam_results) + 2, rows * cols):\n", "        axes[i].axis('off')\n", "\n", "    plt.tight_layout()\n", "    plt.savefig(os.path.join(output_dir, filename))\n", "    print(f\"Saved comparison to {output_dir}/{filename}\")\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def base_detect_plot(model, results, img_path):\n", "\n", "    # preprocess image\n", "    img = cv2.imread(img_path)\n", "    img = cv2.resize(img, (640, 640))\n", "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "    img_float = np.float32(rgb_img) / 255.0\n", "\n", "    #show original image\n", "    plt.figure(figsize=(10, 10))\n", "    plt.imshow(rgb_img)\n", "    plt.axis('off')\n", "    plt.title(\"Original Image\")\n", "\n", "    #show detections\n", "    det_img = results[0].plot()\n", "    plt.figure(figsize=(10, 10))\n", "    plt.imshow(det_img)\n", "    plt.axis('off')\n", "    plt.title(\"YOLO Detections\")\n", "\n", "    return rgb_img, det_img"]}, {"cell_type": "markdown", "metadata": {"id": "DhnitFnYdr6V"}, "source": ["## Main Function\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def test_cam(model, img_path, output_dir=\"cam_results\"):\n", "\n", "    # preprocess image\n", "    img = cv2.imread(img_path)\n", "    img = cv2.resize(img, (640, 640))\n", "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "    img_float = np.float32(rgb_img) / 255.0\n", "    os.makedirs(output_dir, exist_ok=True)\n", "\n", "    # Layers to test based on YOLO v8 architecture and resource/AI recommendations\n", "    layers_to_test = [\n", "        (16, model.model.model[16]),  # Conv layer\n", "        (19, model.model.model[19]),  # Conv layer\n", "        (6, model.model.model[6]),    # Feature layer\n", "        (8, model.model.model[8])     # Feature layer\n", "    ]\n", "    cam_results = []\n", "    #call other functions based on layers\n", "    for idx, layer in layers_to_test:\n", "        print(f\"\\nTesting CAM on layer {idx}: {layer.__class__.__name__}\")\n", "\n", "        cam = create_cam(model, layer, img_float)\n", "        cam_image = visualize_cam(img_float, cam, use_rgb=True)\n", "        filename = f\"cam_layer_{idx}.jpg\"\n", "\n", "        print(f\"Saved CAM for layer {idx} to {filename}\")\n", "\n", "        cam_results.append({\n", "            'layer_idx': idx,\n", "            'layer_name': layer.__class__.__name__,\n", "            'image': cam_image,\n", "            'filename': filename\n", "        })\n", "    return cam_results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#mount drive\n", "from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "model_path = \"/content/drive/MyDrive/Capstone/explainable models/best.pt\"  # Path to your model\n", "image_path = \"/content/drive/MyDrive/Capstone/explainable models/images/n015-2018-07-18-11-41-49+0800__CAM_BACK__1531885800937525.jpg\"  # Path to image\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model, results = run_model(model_path, image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rgb_image, detect_image = base_detect_plot(model, results, image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output = test_cam(model, image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_grid(rgb_image, detect_image, output, output_dir=\"cam_results\")"]}, {"cell_type": "markdown", "metadata": {"id": "sHMAnyd34H-I"}, "source": ["# Section 2: Occluded Cone Analysis for CAM\n", "\n", "This section will break down the visible cones and the occluded cones and how CAM focus on them."]}, {"cell_type": "markdown", "metadata": {"id": "LbIiaFIT8ufy"}, "source": ["## Visible Cones\n", "\n", "First we will analyze the cones labeled 80-100% visible by the Nuscenes dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "model_path = \"/content/drive/MyDrive/Capstone/explainable models/best.pt\"  # Path to your model\n", "image_path = \"/content/drive/MyDrive/Capstone/explainable models/images/n008-2018-08-01-15-16-36-0400__CAM_BACK_RIGHT__1533151427028113.jpg\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model, results = run_model(model_path, image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rgb_image, detect_image =base_detect_plot(model, results, image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output = test_cam(model, image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_grid(rgb_image, detect_image, output, output_dir=\"cam_results\")"]}, {"cell_type": "markdown", "metadata": {"id": "Y-gnjhaS_ywQ"}, "source": ["We analyze 4 different Class Activation Mapping layers. This is due to us finetuning YOLO so the architecture's weights may be different looking for a traffic cone then the traditional YOLO model. We see layer 16 and 19 perform the best as these are deeper in the architechture.\n", "\n", "The better performance of Layer 16 likely stems from its position in YOLOv8's architecture. Layer 16 is deeper in the convolutional network, so it benefits from more complex feature representations while maintaining the spatial resolution necessary for precise localization. The convolutional layer also provides the ability to integrate both low-level features (edges, colors) and higher-level semantic understanding. The other layers show poorer performance both in accuracy and response. Layers 6 and 8 (C2f) show more diffuse activations, indicating they're still processing basic features like edges and textures with less object-specific understanding. Layer 19, a deep layer, shows poorer performance than its earlier layer 16. Layer 19 may encode overly abstract representations that sacrifice fine-grained spatial information so that it may be tuned for larger objects or global scene understanding. We will keep this in mind as we assess the occluded cone.\n", "\n", "We see strong responses on the cones in 16 and 18. Specificall in 16 the cone shape seems to really shine through which is a good sign. We see strong response in the form of red in both these alyers as well. In the early layers we see either incorrect strong responses or minimal response. These layers were probably processing and understand the broad feature map of the image.\n"]}, {"cell_type": "markdown", "metadata": {"id": "dCU0NAlH_P98"}, "source": ["## Occluded Cone Analysis\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "model_path = \"/content/drive/MyDrive/Capstone/explainable models/best.pt\"  # Path to your model\n", "image_path = \"/content/drive/MyDrive/Capstone/explainable models/images/n015-2018-07-11-11-54-16+0800__CAM_BACK_LEFT__1531281493197423.jpg\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model, results = run_model(model_path, image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rgb_image, detect_image =base_detect_plot(model, results, image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output = test_cam(model, image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_grid(rgb_image, detect_image, output, output_dir=\"cam_results\")"]}, {"cell_type": "markdown", "metadata": {"id": "X5hUKDhMBE-p"}, "source": ["## Occluded Cone Discussion\n", "\n", "This shows great results for layer 16 and 19! We see the cone shape very specifically with layer 16 getting the cone pointy shap exactly. It even avoids the gap in between with no response there. There is some diffuse response on the builing within these layer which is interesting. It could be due to the similar circular nature of the building columns or just YOLO assessing the image. However we see a similar poor response inlayer 6 and 8."]}, {"cell_type": "markdown", "metadata": {"id": "zqc25lbvBg-h"}, "source": ["## Overall Discussion\n", "\n", "CAM shows accurate and strong response on both the occluded and visible cones this is a promising result. We also canclearly say that layer 16 is a good layer to assess YOLO's performance for detecting traffic cones. In addition this is the simple CAM method, where there are more complex saliency methods like GRAD-CAM, Score CAM, etc. The CAM methos performs well as is, so with more advanced methods it would be iteresting to see the improved performance. This code took about a minute to run completely on a L4 GPU so very quick compared to the other methods, and more accurate. Howver we are unsure of specific response strength or the contributing specific pixels like we do in other methods."]}, {"cell_type": "markdown", "metadata": {"id": "sHkxVw4-Z13B"}, "source": ["## References\n", "\n", "https://paperswithcode.com/method/cam\n", "\n", "https://arxiv.org/abs/1512.04150\n", "\n", "\n", "https://zilliz.com/learn/class-activation-mapping-CAM\n", "\n", "https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n", "\n", "https://arxiv.org/abs/1910.01279\n", "\n", "https://github.com/rigvedrs/YOLO-V11-CAM\n", "\n", "https://github.com/orgs/ultralytics/discussions/1985\n", "\n", "https://github.com/ultralytics/ultralytics/issues/2020\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}