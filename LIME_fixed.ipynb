{"cells": [{"cell_type": "markdown", "metadata": {"id": "D3lhDtB9WQSs"}, "source": ["# Explainable AI Final Project LIME Implmentation\n", "### Katie Hucker (kh509)\n", "\n", "In this notebook we have the implmentation code to run LIME interpretations for Nuscenes traffic cone images. The LIME interpretations are ran on a finetuned YOLOv8 model that was trained to detect the traffic cones within the dataset.\n", "\n", "The first section defines the code: functions, process, the HOW we get the LIME interpretations.\n", "\n", "Then we compare 2 visibility level traffic cones: occluded (1-40% visible) and visible (80-100%). This is important to my Capstone groups analysis of how YOLO performs for very occluded traffic cones.  "]}, {"cell_type": "markdown", "metadata": {"id": "FtJRWOOYPynh"}, "source": ["[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lZsIpz-v9Sx-ZUYcY8clLkIFKvkO5fwu?usp=sharing)"]}, {"cell_type": "markdown", "metadata": {"id": "gEzLMyIDCFgr"}, "source": ["# Section 1: The Code - How can we implement LIME on our dataset?"]}, {"cell_type": "markdown", "metadata": {"id": "o0nPIL9MWa__"}, "source": ["## Installs Imports and Mounting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#installs uncomment if you do NOT need\n", "!pip install ultralytics lime torch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from PIL import Image\n", "import torch\n", "from ultralytics import YOLO\n", "import lime\n", "from lime import lime_image\n", "from skimage.segmentation import mark_boundaries\n", "import cv2\n", "from skimage.segmentation import mark_boundaries, felzenszwalb, slic, quickshift"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#mount drive\n", "from google.colab import drive\n", "drive.mount('/content/drive')\n", "\n", "# Set device\n", "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n", "print(f\"Using device: {device}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Imports and Configs\n", "model_path = \"/content/drive/MyDrive/Capstone/explainable models/best.pt\"  # Path to your model\n", "image_path = \"/content/drive/MyDrive/Capstone/explainable models/images/n015-2018-07-18-11-41-49+0800__CAM_BACK__1531885800937525.jpg\"  # Path to image"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load model\n", "print(f\"Loading YOLOv8 model from: {model_path}\")\n", "model = YOLO(model_path)\n", "model.to(device)\n", "print(f\"Model loaded with {len(model.names)} classes: {model.names}\")"]}, {"cell_type": "markdown", "metadata": {"id": "0O3denPKFYNl"}, "source": ["## Set-Up Functions"]}, {"cell_type": "markdown", "metadata": {"id": "XyHAgrlOQGXP"}, "source": ["This function runs the YOLO model on the input Nuscenes image. It extracts the detected class IDs and their confidence scores, sorts them in descending order by confidence, and returns the detections along with the raw results and original image."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cone_detect(model, image,  conf_threshold=0.01):\n", "    img_np = image\n", "    results = model(img_np, conf=conf_threshold)\n", "\n", "    # Extract detected classes\n", "    result = results[0]\n", "    detected_classes = []\n", "\n", "    for box in result.boxes:\n", "        class_id = int(box.cls[0].item())\n", "        confidence = box.conf[0].item()\n", "        detected_classes.append((class_id, confidence))\n", "\n", "    # Sort by confidence\n", "    detected_classes.sort(key=lambda x: x[1], reverse=True)\n", "\n", "\n", "    return detected_classes, results, img_np"]}, {"cell_type": "markdown", "metadata": {"id": "2ZqI0nGNQN-n"}, "source": ["This function segments the input image into superpixels using a selected method such as SLIC, Felzenszwalb, or Quickshift. These segments are used by LIME to perturb and explain specific regions of the image relevant to the model's predictions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_segmentation(image, method='slic', n_segments=50, compactness=15, sigma=1.5):\n", "  # https://scikit-image.org/docs/stable/api/skimage.segmentation.html\n", "    if method == 'slic':\n", "        segments = slic(image, n_segments=n_segments, compactness=compactness,\n", "                        sigma=sigma, start_label=1)\n", "    elif method == 'felzenszwalb':\n", "        segments = felzenszwalb(image, scale=100, sigma=sigma, min_size=50)\n", "    elif method == 'quickshift':\n", "        segments = quickshift(image, kernel_size=3, max_dist=6, ratio=0.5)\n", "\n", "    return segments"]}, {"cell_type": "markdown", "metadata": {"id": "pk_y3dk2G-hm"}, "source": ["## LIME Definitions"]}, {"cell_type": "markdown", "metadata": {"id": "cB7Mc5AgQRTG"}, "source": ["These functions initialize and return a LIME image explainer object and the results. The LIME image explainer has a specified kernel width, which controls the locality of the explanation. A smaller kernel width makes the explanation more sensitive to closer perturbations of the input image. Then we use this object to generates a LIME explanation for the given image by applying random perturbations over the provided segments and using the model's prediction function.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_lime_explainer(kernel_width=0.5):\n", "\n", "    return lime_image.LimeImageExplainer(kernel_width=kernel_width)\n", "\n", "def get_lime_explanation(explainer, image, predict_function, segments, top_labels=1, num_samples=500, batch_size=16):\n", "    \"\"\"\n", "    Get LIME explanation for an image.\n", "\n", "    \"\"\"\n", "    explanation = explainer.explain_instance(\n", "        image,\n", "        predict_function,\n", "        top_labels=top_labels,\n", "        hide_color=0,\n", "        num_samples=num_samples,\n", "        batch_size=batch_size,\n", "        segmentation_fn=lambda x: segments\n", "    )\n", "    return explanation"]}, {"cell_type": "markdown", "metadata": {"id": "2Ri6NcKdJZXx"}, "source": ["## LIME Implementation"]}, {"cell_type": "markdown", "metadata": {"id": "3sfFW3Q4Qs-U"}, "source": ["This function is used to generate predictions for LIME by combining what the YOLO model detects and where those detections happen in the image. It uses the segments to guide the LIME explanations It follows these steps:\n", "  1. For each image, it runs YOLO to get the detected objects and their confidence scores.\n", "  2. It breaks the image into a grid and builds a heatmap showing which parts of the image each class was detected in. This helps keep track of both the confidence of a detection and its location.\n", "  3. The result is a prediction vector for each image that reflects how confident the model is about each class, taking into account both what it saw and where it saw it. This makes LIME explanations more meaningful by adding spatial context."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import cv2\n", "from lime import lime_image\n", "import skimage"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict_fn(images, model, class_names):\n", "  # This function was edited from a 2025 MIDS Capstone project generated by me to display confidences and detected box.\n", "  # It previously worked with multiple object classes so some of that functionality is still there.\n", "\n", "    batch_preds = []\n", "\n", "    for img in images:\n", "        #int type check\n", "        img_for_model = img.astype(np.uint8)\n", "\n", "        # predict\n", "        results = model(img_for_model)\n", "        result = results[0]\n", "\n", "        # Get the image dimensions\n", "        img_height, img_width = img.shape[:2]\n", "\n", "        #inits\n", "        grid_size = 20\n", "        confidence_array = np.zeros(len(class_names))\n", "        heatmap = np.zeros((grid_size, grid_size, len(class_names)))\n", "\n", "        # For each box in image\n", "        for box in result.boxes:\n", "            class_id = int(box.cls[0].item())\n", "            confidence = box.conf[0].item()\n", "\n", "            # get the coordinates of the detected box\n", "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n", "\n", "            # Claude Sonnet 3.7 generated this code on 2/13/2025\n", "            grid_x1 = int((x1 / img_width) * grid_size)\n", "            grid_y1 = int((y1 / img_height) * grid_size)\n", "            grid_x2 = int((x2 / img_width) * grid_size)\n", "            grid_y2 = int((y2 / img_height) * grid_size)\n", "\n", "            # Claude Sonnet 3.7 generated this code on 2/13/2025\n", "            grid_x1 = max(0, min(grid_x1, grid_size-1))\n", "            grid_y1 = max(0, min(grid_y1, grid_size-1))\n", "            grid_x2 = max(0, min(grid_x2, grid_size-1))\n", "            grid_y2 = max(0, min(grid_y2, grid_size-1))\n", "\n", "            # Claude Sonnet 3.7 generated this code on 2/13/2025\n", "            for gx in range(grid_x1, grid_x2+1):\n", "                for gy in range(grid_y1, grid_y2+1):\n", "                    heatmap[gy, gx, class_id] = max(heatmap[gy, gx, class_id], confidence)\n", "\n", "            # Update the overall confidence\n", "            confidence_array[class_id] = max(confidence_array[class_id], confidence)\n", "\n", "\n", "        for class_id in range(len(class_names)):\n", "            if confidence_array[class_id] > 0:\n", "                # The presence of the class contributes to the prediction\n", "                confidence_array[class_id] = confidence_array[class_id] * np.mean(heatmap[:,:,class_id] > 0)  # Claude Sonnet 3.7 was used to generate this line of code on 2/13/2025\n", "\n", "        batch_preds.append(confidence_array)\n", "\n", "    return np.array(batch_preds)\n"]}, {"cell_type": "markdown", "metadata": {"id": "pjLlqsWXSFVm"}, "source": ["This function ties everything together to generate the LIME explanation for an our image and using the provided YOLO detection model.\n", "\n", "  1. It calls the prediction wrapper first we defined this above.\n", "  2. It then segments the image using a method like SLIC to create superpixels, which are important for LIME to know what parts of the image to modify.\n", "  3. The LIME explainer runs to get an explanation of which parts of the image most influenced the model\u2019s prediction.\n", "  4. The function returns the explanation, the segmentation used, and the singular detected class."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def analyze_image(image, model, class_names, segmentation_method='slic',\n", "                 n_segments=50, num_samples=1000, batch_size=16):\n", "\n", "    # defined above\n", "    def predict_wrapper(images):\n", "        return predict_fn(images, model, class_names)\n", "\n", "    # defined above\n", "    segments = create_segmentation(image, method=segmentation_method, n_segments=n_segments)\n", "\n", "    # defined above\n", "    explainer = create_lime_explainer()\n", "\n", "    # defined above\n", "    explanation = get_lime_explanation( # https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.explanation\n", "        explainer, image, predict_wrapper, segments,\n", "        num_samples=num_samples, batch_size=batch_size\n", "    )\n", "\n", "    if len(explanation.top_labels) > 0:\n", "        top_class_id = explanation.top_labels[0]\n", "\n", "\n", "    return explanation, segments, top_class_id\n"]}, {"cell_type": "markdown", "metadata": {"id": "ST1giWePJJ3t"}, "source": ["## Plotting and Output Code\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "def plot_lime_basic(image, explanation, top_class_id, class_names, figsize=(15, 5)):\n", "\n", "    if isinstance(class_names, (list, tuple, np.ndarray)) and len(class_names) > top_class_id:\n", "        class_name = class_names[top_class_id]\n", "    else:\n", "        class_name = f\"Class {top_class_id}\"  # Claude Sonnet 3.7 was used to generate this line of Code on 4/15/2025\n", "\n", "\n", "    temp, mask = explanation.get_image_and_mask(  #https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.explanation\n", "        top_class_id,\n", "        positive_only=False,\n", "        num_features=10,\n", "        hide_rest=False\n", "    )\n", "\n", "    plt.figure(figsize=figsize)\n", "\n", "    #Regular image\n", "    plt.subplot(1, 2, 1)\n", "    plt.imshow(image)\n", "    plt.title(\"Original Image\")\n", "    plt.axis('off')\n", "\n", "    # Lime image with the segment boundaries\n", "    plt.subplot(1, 2, 2)\n", "    plt.imshow(mark_boundaries(temp / 255.0, mask))\n", "    plt.title(f\"LIME Explanation\\nClass: {class_name}\")\n", "    plt.axis('off')\n", "\n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "\n", "\n", "def enhanced_visualize_segments(image, explanation, label, threshold=0.01):\n", "    ## Claude Sonnet 3.7 was used to generate this function to show the segments with numbers and the boundaries on the segments for more detailed analuysis\n", "    ## Claude was asked \"Please add numbers to the segments and provide borders to all the images and then provided the current working LIME code including the plot code above.\"\n", "    ## This was completed on 4/17/2025 at 4:23pm.\n", "\n", "    # Check if the label exists in the explanation\n", "    if label not in explanation.local_exp:\n", "        # If no explanation for this label, return original image\n", "        return np.copy(image).astype(np.float32) / 255\n", "\n", "    dict_heatmap = dict(explanation.local_exp[label])\n", "    segments = explanation.segments\n", "\n", "    # Normalize values for visualization\n", "    values = np.array(list(dict_heatmap.values()))\n", "    if len(values) == 0:\n", "        return np.copy(image).astype(np.float32) / 255\n", "\n", "    abs_values = np.abs(values)\n", "    max_abs_val = abs_values.max() if abs_values.max() > 0 else 1.0  # Avoid division by zero\n", "\n", "    # Create a copy of the image\n", "    segment_image = np.copy(image).astype(np.float32) / 255\n", "\n", "    # Create an overlay for segments\n", "    overlay = np.zeros_like(segment_image)\n", "\n", "    # Create a mask for all significant segments\n", "    significant_mask = np.zeros(segments.shape, dtype=bool)\n", "\n", "    # Color each segment based on importance\n", "    for segment_id, value in dict_heatmap.items():\n", "        # Skip segments with minimal contribution\n", "        if abs(value) / max_abs_val < threshold:\n", "            continue\n", "\n", "        # Normalize value between -1 and 1\n", "        normalized_value = value / max_abs_val\n", "\n", "        # Choose color based on contribution (positive=green, negative=red)\n", "        if normalized_value > 0:\n", "            # Increase the intensity for better visibility\n", "            color = [0, min(1.0, normalized_value * 2), 0]  # Green for positive\n", "        else:\n", "            # Increase the intensity for better visibility\n", "            color = [min(1.0, -normalized_value * 2), 0, 0]  # Red for negative\n", "\n", "        # Set color for the segment\n", "        segment_mask = segments == segment_id\n", "        overlay[segment_mask] = color\n", "        significant_mask |= segment_mask\n", "\n", "    # Blend original image with overlay, with increased contrast\n", "    alpha = 0.7  # Higher transparency factor\n", "\n", "    # Slightly dim parts of the image not in significant segments\n", "    segment_image[~significant_mask] *= 0.7\n", "\n", "    # Apply the colored overlay only on significant segments\n", "    blended = segment_image.copy()\n", "    if np.any(significant_mask):  # Only if we have significant segments\n", "        blended[significant_mask] = (1 - alpha) * segment_image[significant_mask] + alpha * overlay[significant_mask]\n", "\n", "    return np.clip(blended, 0, 1)\n", "\n", "\n", "def plot_enhanced_visualization(image, segments, explanation, top_class_id, class_names, figsize=(18, 6), show_segment_numbers=True):\n", "    ## Using the same prompt and time this function was generated with Claude Sonnet see above citation.\n", "\n", "\n", "    if isinstance(class_names, (list, tuple, np.ndarray)) and len(class_names) > top_class_id:\n", "        class_name = class_names[top_class_id]\n", "    else:\n", "        class_name = f\"Class {top_class_id}\"\n", "\n", "    #calls above function\n", "    enhanced_viz = enhanced_visualize_segments(image, explanation, top_class_id, threshold=0.01)\n", "\n", "    plt.figure(figsize=figsize)\n", "\n", "\n", "    plt.subplot(1, 3, 1)\n", "    plt.imshow(image)\n", "    plt.title(\"Original Image\")\n", "    plt.axis('off')\n", "\n", "\n", "    plt.subplot(1, 3, 2)\n", "    plt.imshow(mark_boundaries(image/255.0, segments))\n", "    plt.title(f\"Segmentation\")\n", "    plt.axis('off')\n", "\n", "    # Add segment numbers\n", "    if show_segment_numbers:\n", "\n", "        unique_segments = np.unique(segments)\n", "\n", "        from scipy import ndimage\n", "\n", "        for segment_id in unique_segments:\n", "\n", "            mask = segments == segment_id\n", "\n", "            if np.sum(mask) < 50:\n", "                continue\n", "\n", "            cy, cx = ndimage.center_of_mass(mask)\n", "\n", "            plt.text(cx, cy, str(segment_id),\n", "                     color='white', fontsize=8, ha='center', va='center',\n", "                     bbox=dict(facecolor='black', alpha=0.5, pad=0))\n", "\n", "\n", "    plt.subplot(1, 3, 3)\n", "    plt.imshow(enhanced_viz)\n", "    plt.title(f\"Enhanced LIME Visualization for {class_name}\\nGreen = Positive, Red = Negative\")\n", "    plt.axis('off')\n", "\n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "    # I added this code\n", "    if top_class_id in explanation.local_exp:\n", "\n", "        dict_heatmap = dict(explanation.local_exp[top_class_id])\n", "        segment_importances = sorted(dict_heatmap.items(), key=lambda x: abs(x[1]), reverse=True)\n", "\n", "        # Which segments have largest response sort then print\n", "        print(\"\\nMost important segments:\")\n", "        for segment_id, importance in segment_importances[:5]:\n", "            direction = \"Positive\" if importance > 0 else \"Negative\"\n", "            print(f\"Segment {segment_id}: {direction} contribution of {abs(importance):.4f}\")\n", "\n", "    return\n", "\n", "\n", "def print_explanation_summary(explanation, top_class_id, class_names):\n", "    \"\"\"\n", "    Print numerical data like above but more detailed\n", "    \"\"\"\n", "    if isinstance(class_names, (list, tuple, np.ndarray)) and len(class_names) > top_class_id:\n", "        class_name = class_names[top_class_id]\n", "    else:\n", "        class_name = f\"Class {top_class_id}\"\n", "    dict_heatmap = dict(explanation.local_exp[top_class_id])\n", "    segment_importances = sorted(dict_heatmap.items(), key=lambda x: abs(x[1]), reverse=True)\n", "\n", "    # Get the positive and negative contributions\n", "    positive_segs = [s for s in segment_importances if s[1] > 0][:5]\n", "    negative_segs = [s for s in segment_importances if s[1] < 0][:5]\n", "\n", "    if positive_segs:\n", "        print(\"\\nTop Positive Contributing Segments:\")\n", "        for segment, importance in positive_segs:\n", "            print(f\"- Segment {segment}: contribution of {importance:.4f}\")\n", "\n", "    if negative_segs:\n", "        print(\"\\nTop Negative Contributing Segments:\")\n", "        for segment, importance in negative_segs:\n", "            print(f\"- Segment {segment}: contribution of {importance:.4f}\")\n", "\n", "    # Claude 3.7 generated this math code on 4/17/2025 on 4:56pm\n", "    if positive_segs or negative_segs:\n", "        total_positive = sum(imp for _, imp in positive_segs)\n", "        total_negative = sum(imp for _, imp in negative_segs)\n", "        total_abs = abs(total_positive) + abs(total_negative)\n", "        pos_percentage = (abs(total_positive) / total_abs) * 100 if total_abs > 0 else 0\n", "        neg_percentage = (abs(total_negative) / total_abs) * 100 if total_abs > 0 else 0\n", "\n", "        print(f\"\\nPositive contributions account for approximately {pos_percentage:.1f}% of total influence\")\n", "        print(f\"Negative contributions account for approximately {neg_percentage:.1f}% of total influence\")"]}, {"cell_type": "markdown", "metadata": {"id": "e-361jaeJiBM"}, "source": ["## Call functions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#set-up image\n", "img = Image.open(image_path).convert(\"RGB\")\n", "img_np = np.array(img)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["detected_classes, results, img_np = cone_detect(model, img_np)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["segments = create_segmentation(img_np)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explanation, segments, class_names = analyze_image(img_np, model, ['traffic_cone'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["top_class_id = detected_classes[0][0]\n", "plot_lime_basic(img_np, explanation, top_class_id, class_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "plot_enhanced_visualization(img_np, segments, explanation, top_class_id, class_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print_explanation_summary(explanation, top_class_id, class_names)"]}, {"cell_type": "markdown", "metadata": {"id": "8hweamHpCNlM"}, "source": ["# Section 2: Occluded Cone Analysis"]}, {"cell_type": "markdown", "metadata": {"id": "6w1JLyZkCb49"}, "source": ["## Visible Cone"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Imports and Configs\n", "model_path = \"/content/drive/MyDrive/Capstone/explainable models/best.pt\"  # Path to your model\n", "image_path = \"/content/drive/MyDrive/Capstone/explainable models/images/n008-2018-08-01-15-16-36-0400__CAM_BACK_RIGHT__1533151427028113.jpg\" # Path to image"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#set-up image\n", "img = Image.open(image_path).convert(\"RGB\")\n", "img_np = np.array(img)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["detected_classes, results, img_np = cone_detect(model, img_np)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["segments = create_segmentation(img_np)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explanation, segments, class_names = analyze_image(img_np, model, ['traffic_cone'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["top_class_id = detected_classes[0][0]\n", "plot_lime_basic(img_np, explanation, top_class_id, class_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "plot_enhanced_visualization(img_np, segments, explanation, top_class_id, class_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print_explanation_summary(explanation, top_class_id, class_names)"]}, {"cell_type": "markdown", "metadata": {"id": "t5Goa-D9Iy6c"}, "source": ["## Discussion\n", "\n", "We ran the LIME samples for 1000 iterations with 50 segments at batch size 16.  I wanted to keep the methods somewhat equal in this sense, however,LIME crashed at 5000 samples with A100 GPU so I went down to 1000. We see above that the positive contributions in green are more focused on the cones and the red shows up on that in between part which is a good sign that YOLO is recognizing thats when the cone ends and the concrete begins. The contribution numerics are very small to the point where we ask ourselves why that is so sensitive but its still able to detect the visible cones. LIME sees more positive contribution than other methods. More specifically we see the the van colored in positive contribution for YOLO. I wonder if this is due to the color its recognizing this NOT a traffic cone so its helping the deteciton. The numbered segments give is more detail on specific regions which help or don't help."]}, {"cell_type": "markdown", "metadata": {"id": "RTcayhL-CgxH"}, "source": ["## Occluded Cone"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["image_path=\"/content/drive/MyDrive/Capstone/explainable models/images/n015-2018-07-11-11-54-16+0800__CAM_BACK_LEFT__1531281493197423.jpg\"\n", "img = Image.open(image_path).convert(\"RGB\")\n", "img_np = np.array(img)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["detected_classes, results, img_np = cone_detect(model, img_np)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["segments = create_segmentation(img_np)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class_names = ['traffic_cone']\n", "def predict_wrapper(images):\n", "    return predict_fn(images, model, class_names )\n", "\n", "# Get LIME explanation\n", "explainer = create_lime_explainer()\n", "explanation = get_lime_explanation(explainer, img_np, predict_wrapper, segments)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["top_class_id = detected_classes[0][0]\n", "plot_lime_basic(img_np, explanation, top_class_id, class_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "plot_enhanced_visualization(img_np, segments, explanation, top_class_id, class_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print_explanation_summary(explanation, top_class_id, class_names)"]}, {"cell_type": "markdown", "metadata": {"id": "CqCfYrjvJ3Tr"}, "source": ["## Occluded Cone Discussion"]}, {"cell_type": "markdown", "metadata": {"id": "gAue0hgUOoMm"}, "source": ["LIME shows a lot of positive contributions in the occluded cone images, however, the strongest are where the cones are. I feel like this image as more response in general than the other 2 and all we really gain is that LIME did not like the drain pipe region, but otherwise it found the cones with positive response as well as other parts of the image."]}, {"cell_type": "markdown", "metadata": {"id": "j5ob6sk-J5kd"}, "source": ["## Overall Discussion"]}, {"cell_type": "markdown", "metadata": {"id": "GDFJu71SO9xL"}, "source": ["LIME took a long time to run at 1000 samples, it was ran with segmentations to try to find regions of interest and though this is beneficial when we see the red segment in the visible cones, we see a lot of positive segments in both occluded and visible. There is really no difference between the cone visibility and this method. In the the occlded cones even thougbh section 35 does not have a cone at all LIME is showing a strong response there with YOLO like the neighboring sections.\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "WKmScXEpWmBZ"}, "source": ["## References\n", "Claude Sonnet 3.7 was used when needed especially with plotting. It is cited throughout.\n", "\n", "\n", "https://lime-ml.readthedocs.io/en/latest/lime.html\n", "\n", "https://github.com/ultralytics/yolov5/issues/1991\n", "\n", "https://medium.com/@shreeraj260405/hands-on-lime-practical-implementation-for-image-text-and-tabular-data-95566da87f57\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}