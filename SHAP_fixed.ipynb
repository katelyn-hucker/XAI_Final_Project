{"cells": [{"cell_type": "markdown", "metadata": {"id": "FC6fOQn2GgWO"}, "source": ["# Explainable AI Final Project SHAP Implementation and Occluded Traffic Cone Analysis\n", "### Katie Hucker (kh509)\n", "\n", "In this notebook we have the implmentation code to run SHAP interpretations for Nuscenes traffic cone images. The SHAP interpretations are ran on a finetuned YOLOv8 model that was trained to detect the traffic cones within the dataset.\n", "\n", "The first section defines the code: functions, process, the HOW we get the SHAP interpretations.\n", "\n", "Then we compare 2 visibility level traffic cones: occluded (1-40% visible) and visible (80-100%). This is important to my Capstone groups analysis of how YOLO performs for very occluded traffic cones."]}, {"cell_type": "markdown", "metadata": {"id": "UgWitAbn72A7"}, "source": ["[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Kild6vkcLObUuLfUw7teoZSu2-OIp2m0?usp=sharing)"]}, {"cell_type": "markdown", "metadata": {"id": "UOCrN_WW1AJP"}, "source": ["# Section 1: SHAP Implementation"]}, {"cell_type": "markdown", "metadata": {"id": "sjFgXCrRJr2i"}, "source": ["## Imports Installs and Mounting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#installs uncomment if you need\n", "#!pip install ultralytics shap torch torchvision"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import numpy as np\n", "import shap\n", "from ultralytics import YOLO\n", "from PIL import Image\n", "import matplotlib.pyplot as plt\n", "import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#mount drive\n", "from google.colab import drive\n", "drive.mount('/content/drive')\n", "\n", "# Set device\n", "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n", "#print(f\"Using device: {device}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Imports and Configs\n", "model_path = \"/content/drive/MyDrive/Capstone/explainable models/best.pt\"  # Path to your model\n", "image_path = \"/content/drive/MyDrive/Capstone/explainable models/images/n015-2018-07-18-11-41-49+0800__CAM_BACK__1531885800937525.jpg\"  # Path to image\n", "n_evals = 1000  # Number of SHAP evaluations # bigger number is more accurate\n", "stride =  64 # this will downsample the image to 64 x 64 change if you want\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load model\n", "print(f\"Loading YOLOv8 model from: {model_path}\")\n", "model = YOLO(model_path)\n", "#model.to(device)\n", "print(f\"Model loaded with {len(model.names)} classes: {model.names}\")"]}, {"cell_type": "markdown", "metadata": {"id": "xgjum09hJvpG"}, "source": ["## Set-Up Functions"]}, {"cell_type": "markdown", "metadata": {"id": "THnczCnyLcIc"}, "source": ["This function below predicts the traffic cones from YOLO and prepares the image to work with SHAP. Since we only have ONE class thats all we have to worry about. We are just looking for multiple traffic cone detections within an image.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cone_detect(image_path):\n", "    \"\"\"detect cone\"\"\"\n", "\n", "    img = Image.open(image_path).convert(\"RGB\")\n", "    print(f\"Original image size: {img.size}\")\n", "\n", "    #uses resize image function seen below\n", "    img_resized = resize_image(img, size=(640, 640))\n", "    img_np = np.array(img_resized)\n", "\n", "    # inference\n", "    results = model(img_np, conf=0.01)\n", "\n", "    # print results\n", "    num_detections = len(results[0].boxes)\n", "    print(f\"Detected {num_detections} traffic cones\")\n", "    print(\"Confidence scores:\", [f\"{conf:.3f}\" for conf in results[0].boxes.conf.tolist()])\n", "\n", "    return results, img_np"]}, {"cell_type": "markdown", "metadata": {"id": "k80eS7BNX6cz"}, "source": ["YOLO and SHAP require a specific set-up this function resizes the image to a size of 640x640."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def resize_image(img, size=(640, 640)):\n", "\n", "    width, height = size\n", "    new_width = (width // stride) * stride\n", "    new_height = (height // stride) * stride\n", "\n", "    img_resized = img.resize((new_width, new_height), Image.LANCZOS)\n", "    print(f\"Image resized to {new_width}x{new_height}\")\n", "\n", "    return img_resized"]}, {"cell_type": "markdown", "metadata": {"id": "CblW_uKOzcG5"}, "source": ["## SHAP Functions"]}, {"cell_type": "markdown", "metadata": {"id": "KS72hhXC1x2K"}, "source": ["This function is a prediction wrapper compatible with SHAP for interpreting the YOLO traffic cone detections. The function performs inference on the provided image using the model provided. For each image, it keeps track of the confidence scores, storing them in a SHAP compatible tensor. Since we are only interested in one potential class: traffic cones \u2014 this function simplifies output processing by summing the detection confidences for that single class across each image in the batch."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_prediction_function():\n", "    \"\"\"Create a prediction function for SHAP\"\"\"\n", "    def predict_fn(images):\n", "\n", "        if isinstance(images, np.ndarray):\n", "            #normalization\n", "            if images.max() > 1.0:\n", "                images = images.astype(np.float32) / 255.0 #if values are in the 0\u2013255 range, scale them to 0\u20131\n", "\n", "            images = torch.tensor(images).float()\n", "\n", "        #add batch dimension\n", "        #Claude Sonnet 3.7 generated this line at 4/18/2025\n", "        if images.dim() == 3:\n", "            images = images.unsqueeze(0)\n", "\n", "        # Convert to NCHW format if needed\n", "         #Claude Sonnet 3.7 generated this line at 4/18/2025\n", "        if images.shape[-1] == 3:  # If NHWC format\n", "            images = images.permute(0, 3, 1, 2)  # Convert to NCHW\n", "\n", "        # inference\n", "        with torch.no_grad():\n", "            results = model(images.to(device), conf=0.01)\n", "\n", "        # output\n", "        batch_size = images.shape[0]\n", "        output = torch.zeros((batch_size, len(model.names))).to(device)\n", "\n", "        #Claude Sonnet 3.7 generated this for loop at 4/18/2025\n", "        for i, res in enumerate(results):\n", "            boxes = res.boxes\n", "            if len(boxes) > 0:\n", "                for conf, cls_id in zip(boxes.conf, boxes.cls.int()):\n", "                    output[i, cls_id.item()] += conf.item()\n", "\n", "        return output\n", "\n", "    return predict_fn"]}, {"cell_type": "markdown", "metadata": {"id": "mWp_FJF024yQ"}, "source": ["This function generates SHAP values to explain YOLO-based traffic cone detections within a given image. It begins by normalizing the input image. It then calls the prediction wrapper function we defined above. We create a blurring mask for the SHAP interpretation. SHAP needs a way to simulate what the model would predict if certain parts of the image were missing or unknown. SHAP replaces them with a blurred version of the image. This method helps keep the modified image looking natural and avoids introducing artifacts that the model might misinterpret. We create the shap explainer object. Which will then, compute the SHAP values for the traffic cone in the image, evaluating how different image regions contribute to the detection output."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def generate_shap_values(results, img_np):\n", "    \"\"\"Generate SHAP values for the traffic cone detector\"\"\"\n", "\n", "    # normalize again\n", "    img_normalized = img_np.astype(np.float32) / 255.0\n", "\n", "    #call function defined aboe\n", "    predict_fn = create_prediction_function()\n", "\n", "    # mask the image\n", "    masker = shap.maskers.Image(\"blur(16,16)\", img_normalized.shape)\n", "\n", "    # Create explainer\n", "    #https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html\n", "    explainer = shap.Explainer(predict_fn, masker, output_names=list(model.names.values()))\n", "\n", "    print(f\"Generating SHAP explanation with {n_evals} evaluations...\")\n", "\n", "    shap_values = explainer( #used above link to find optimal params\n", "        np.expand_dims(img_normalized, 0),\n", "        max_evals=n_evals,\n", "        batch_size=10,\n", "        outputs=[0]\n", "    )\n", "\n", "    print(\"SHAP values generated\")\n", "\n", "    return {\n", "        \"shap_values\": shap_values,\n", "        \"shap_img\": img_normalized,\n", "        \"detection_results\": results\n", "    }\n"]}, {"cell_type": "markdown", "metadata": {"id": "JJfq-B7dzQMI"}, "source": ["## Plotting Functions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_detection(results):\n", "    \"\"\"YOLO Output\"\"\"\n", "    plt.figure(figsize=(10, 6))\n", "    plt.imshow(results[0].plot())\n", "    plt.title(f\"YOLOv8 Detection ({len(results[0].boxes)} traffic cones)\")\n", "    plt.axis('off')\n", "    plt.show()\n", "\n", "def plot_standard_shap(shap_data):\n", "    \"\"\"Use built in SHAP plot\"\"\"\n", "    # https://shap.readthedocs.io/en/latest/generated/shap.plots.image.html\n", "    shap_img = shap_data[\"shap_img\"]\n", "    shap_values = shap_data[\"shap_values\"]\n", "\n", "    plt.figure(figsize=(10, 6))\n", "\n", "    shap.plots.image(\n", "        shap_values,\n", "        pixel_values=np.expand_dims(shap_img, 0),\n", "        labels=[\"Traffic Cone\"],\n", "        width=18,\n", "        labelpad=5.0,\n", "        vmax=abs(shap_values.values).max(),\n", "        show=True\n", "    )\n", "\n", "def plot_heatmap_overlay(shap_data):\n", "    \"\"\"Make a clearer heatmap\"\"\"\n", "\n", "    shap_img = shap_data[\"shap_img\"]\n", "    shap_values = shap_data[\"shap_values\"]\n", "    shap_vals = shap_values.values[0, :, :, :, 0]\n", "\n", "    #heatmap calculations\n", "    #Claude Sonnet 3.7 was used to generate the heatmap calculation 4/18/2025\n", "    shap_sum = np.abs(shap_vals).sum(axis=-1)\n", "    shap_norm = shap_sum / shap_sum.max()\n", "\n", "    plt.figure(figsize=(10, 6))\n", "\n", "    plt.imshow(shap_img)\n", "\n", "    plt.imshow(shap_norm, cmap='hot', alpha=0.5)\n", "    plt.colorbar(label='SHAP Importance')\n", "    plt.title(\"SHAP Importance Heatmap\")\n", "    plt.axis('off')\n", "    plt.show()\n", "\n", "    return shap_norm\n", "\n", "def plot_combined_visualization(shap_data, shap_norm):\n", "    \"\"\"Plot comparison\"\"\"\n", "\n", "    shap_img = shap_data[\"shap_img\"]\n", "    results = shap_data[\"detection_results\"]\n", "\n", "\n", "    plt.figure(figsize=(14, 6))\n", "\n", "\n", "    plt.subplot(1, 2, 1)\n", "    plt.imshow(results[0].plot())\n", "    plt.title(\"YOLOv8 Detection\")\n", "    plt.axis('off')\n", "\n", "    plt.subplot(1, 2, 2)\n", "    plt.imshow(shap_img)\n", "    plt.imshow(shap_norm, cmap='hot', alpha=0.5)\n", "    plt.title(\"SHAP Importance Heatmap\")\n", "    plt.axis('off')\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "peUxNQrDXrR5"}, "source": ["## Main Function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results, img_np = cone_detect(image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_detection(results)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_data = generate_shap_values(results, img_np)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot standard SHAP visualization\n", "plot_standard_shap(shap_data)\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot heatmap overlay\n", "shap_norm = plot_heatmap_overlay(shap_data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "plot_combined_visualization(shap_data, shap_norm)"]}, {"cell_type": "markdown", "metadata": {"id": "YsEHWnKt1Lh2"}, "source": ["# Section 2: Occlusion Cone Analysis and Experimentation"]}, {"cell_type": "markdown", "metadata": {"id": "Mr9-CwUB0Z1M"}, "source": ["## The Visible Cones\n", "\n", "These cones are labeled as 80-100% visible. Let's see what SHAP says..."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# New image\n", "\n", "image_path = \"/content/drive/MyDrive/Capstone/explainable models/images/n008-2018-08-01-15-16-36-0400__CAM_BACK_RIGHT__1533151427028113.jpg\"  # Path to image\n", "n_evals = 5000  # Number of SHAP evaluations # bigger number is more accurate\n", "stride =  64 # this will downsample the image to 64 x 64 change if you want"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results, img_np = cone_detect(image_path=image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_detection(results)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_data = generate_shap_values(results, img_np)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_standard_shap(shap_data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_norm = plot_heatmap_overlay(shap_data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_combined_visualization(shap_data, shap_norm)"]}, {"cell_type": "markdown", "metadata": {"id": "TydYepv82w1H"}, "source": ["### Visible Cone for SHAP Discussion\n", "\n", "For the two cones which were labeled as very visible in the category bin of 80-100%, we see the SHAP heatmap does focusin on them. Specifically the bottom of the cone is shows a stronger response. I think it is really interesting that the barrier in between the two cones is not focus on at all -- showing that SHAP IS looking at the cones and trying to avoid other things.\n", "\n", "However we see the colorbars in the first two figures which shows a small numeric response in general but within this scale it shows a good enough response to be 'strong'"]}, {"cell_type": "markdown", "metadata": {"id": "g4kfUOqf2PH7"}, "source": ["## Occluded Cone\n", "\n", "Here we look at a cone which as labeled as 1-40% visible in the Nuscenes dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#new image\n", "image_path = \"/content/drive/MyDrive/Capstone/explainable models/images/n015-2018-07-11-11-54-16+0800__CAM_BACK_LEFT__1531281493197423.jpg\"  # Path to image\n", "n_evals = 5000  # Number of SHAP evaluations # bigger number is more accurate\n", "stride =  64 # this will downsample the image to 64 x 64 change if you want"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results, img_np = cone_detect(image_path=image_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_detection(results)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_data = generate_shap_values(results, img_np)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_standard_shap(shap_data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shap_norm = plot_heatmap_overlay(shap_data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_combined_visualization(shap_data, shap_norm)"]}, {"cell_type": "markdown", "metadata": {"id": "LzvGBpiJ2zuz"}, "source": ["### Occluded Cone for SHAP Discussion\n", "\n", "This cones were labeled as 1-40% visible, so very occluded. We see that YOLO fails to even detect them in this run of YOLO (however in previous runs YOLO did detect). BUT this makes it more interesting in that, we see SHAP still is looking at the cones! The color bar shows about HALF as much as the visible cone for the range. So we can interpret we at least need a SHAP greater than (3 * 1e-6 ) to get a detection. It also does not seem as much accurate in a specific location. We see some response on the grassy background and the white building. However, the top of the cone does seem to show some stronger response very mild though."]}, {"cell_type": "markdown", "metadata": {"id": "ysJ5c-L724C8"}, "source": ["## SHAP Discussion"]}, {"cell_type": "markdown", "metadata": {"id": "ptDYTrZU5EE0"}, "source": ["Overall, SHAP does well in showing us where the cone is even if YOLO failed to 'detect' but the numbers and response are very minimal for the occluded cone whencompared to the visible cone. In addition, we see the visible cone image the SHAP response avoids non cone objects even though it is directly inbetween, this is not as clear in the occluded cone image. With some of the grassy middl between the cones shows response. The analysis for 5000 samples on the L4 GPU took under 5 mins for each run which is pretty quick for a wide range of data. I would recommend further analysis in the 'stride' function to see if greater denser super pixels would aid in stronger SHAP response."]}, {"cell_type": "markdown", "metadata": {"id": "K0FLjMUF57MV"}, "source": ["## References for SHAP implementation\n", "\n", "https://www.nuscenes.org/\n", "\n", "https://docs.ultralytics.com/models/yolov8/\n", "\n", "https://shap.readthedocs.io/en/latest/example_notebooks/image_examples/image_classification/Explain%20MobilenetV2%20using%20the%20Partition%20explainer%20%28PyTorch%29.html\n", "\n", "\n", "https://shap.readthedocs.io/en/latest/generated/shap.plots.image.html\n", "\n", "https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install nbconvert"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}